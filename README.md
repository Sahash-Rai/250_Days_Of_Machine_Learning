# A 280-Day Exploration in Machine and Deep Learning: My Learning Odyssey
![Machine Learning](https://github.com/SahashRaee/280_Days_Of_Machine_and_Deep_Learning/blob/main/Assets/Machine_Deep_Learning.png)


## ðŸ“‚ Resources: 
***Research Papers***

| **Day Range** | **Paper**                                                                                     | **Topics Covered**                       |
|---------------|-----------------------------------------------------------------------------------------------|------------------------------------------|
| Days 5â€“20     | [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/afewthings.pdf) | ML foundations, bias-variance tradeoff   |
| Days 20â€“30    | [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) | Supervised, unsupervised learning        |
| Days 30â€“40    | [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)              | CNNs, ResNets                            |
| Days 40â€“50    | [Support Vector Machines: A Practical Guide](https://www.cs.sjsu.edu/~wang/350/Papers/svm.pdf) | SVM, optimization, kernel methods        |
| Days 51â€“55    | [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                                 | Introduction to Transformers, NLP basics |
| Days 56â€“60    | [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)                                | GANs, generative models                  |
| Days 61â€“65    | [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) | Reinforcement learning, AlphaZero        |
| Days 66â€“100   | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) | NLP, transformer-based models            |
| Days 101â€“140  | [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) | Deep learning optimization, model efficiency |
| Days 141â€“180  | [Wide Residual Networks](https://arxiv.org/abs/1605.07146)                                    | Deep residual network optimizations      |
| Days 181â€“220  | [Convolutional Neural Networks at Scale](https://arxiv.org/abs/1605.07146)                    | Advanced CNN architectures               |
| Days 221â€“260  | [A Survey on Deep Learning](https://arxiv.org/abs/2003.03123)                                | Deep learning survey, advanced topics    |
| Days 261â€“280  | [AlphaFold: Using AI for scientific discovery](https://www.nature.com/articles/s41592-021-01078-x) | Deep learning in bioinformatics, innovations in AI |

# Day 1: Supervised Learning - Linear Regression

## Topics Covered:
- Introduction to Supervised Learning
- Linear Regression: Concept and Application
- Understanding the dataset and model fitting
- Evaluating model performance using Mean Squared Error (MSE) and R-squared

## Key Learnings:
- **Supervised Learning** is a type of machine learning where the model learns from labeled data to make predictions or classifications.
- **Linear Regression** is a supervised learning algorithm used for predicting continuous values based on input features.
- We used **Scikit-learn**'s Linear Regression model to predict house prices based on the size of the house.
- We evaluated the model using **Mean Squared Error (MSE)** and **R-squared** metrics to check how well the model performs.

## Code Implementation ðŸ’»:
The following code implements **Linear Regression** on a simple dataset of house sizes and prices.


![Code_Day_1](https://github.com/SahashRaee/280_Days_Of_Machine_and_Deep_Learning/blob/main/Assets/Codes_Snapshot/code_day_1.png)

